{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e932c-322b-49d4-84d8-29fd3c86822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0bfd95-5127-4693-8255-508d9e35a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.What is Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1fe0c9-faeb-4837-92c1-83991b761d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans= Simple linear regression is a method to find relationship between two variable.One independent and one dependent variable.\n",
    "and it fits a straight line though data points so we can predict the output for the given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a457dc95-05aa-4184-9bfc-b11aac671102",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are the key assumptions of Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863a203-24ed-4143-a465-45955ca04ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Linearity: The relationship between the independent and dependent variable is linear (a straight line).\n",
    "Independence: The residuals (differences between observed and predicted values) are independent of each other.\n",
    "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable.\n",
    "Normality of Residuals: The residuals are normally distributed.\n",
    "No Multicollinearity: Since Simple Linear Regression has only one independent variable, multicollinearity does not apply, \n",
    "but this is important in cases with multiple predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79d4a2-a9de-4ba1-a4af-6ba7979ee238",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.What does the coefficient m represent in the equation Y=mX+c?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c523b3-9b4a-4761-ad2f-f1de2dce37b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans= the coefficient m represents the slope of the line. It indicates the rate of change in \n",
    "the dependent variable Y for a one-unit increase in the independent variable X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74bd55-cd26-4e61-8c59-e374991b5332",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.What does the intercept c represent in the equation Y=mX+c?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721eac5-559c-4d40-96e7-891789d640d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=he intercept c represents the Y-intercept of the line. It is the value of Y when X=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d36dea-e0fa-45b4-b29c-e22c168dc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.How do we calculate the slope m in Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699e28b-0242-4066-94bf-8c375a7f8085",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=The slope (m) in Simple Linear Regression is calculated using the following formula:\n",
    "Xi: Individual values of the independent variable.\n",
    "Yi: Individual values of the dependent variable.\n",
    "X: Mean of the independent variable.\n",
    "Y: Mean of the dependent variable.\n",
    "\n",
    "m=sum(Xi-X)(Yi-Y)/sum(Xi-X)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c9361-dce9-49b0-b2ea-f49e85dbc665",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.What is the purpose of the least squares method in Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a2763-4932-4c36-a527-116c8aa2277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=The purpose of the least squares method in Simple Linear Regression is to find the best-fitting line \n",
    "for the data by minimizing the sum of the squared differences between the observed values and the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b18fb-abb5-4023-9d85-27e5f41155ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf71d24-3dcd-4171-87b5-6d2acd1af835",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=The coefficient of determination (R2) in Simple Linear Regression measures the proportion of the variance in the dependent variable \n",
    "(Y) that is explained by the independent variable (X) through the model. It indicates how well the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd20d8c-44f9-4287-9d6f-1e67e4da84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "8.What is Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033de6b-5556-4dfd-8533-7082dbe3c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Multiple Linear Regression is an extension of Simple Linear Regression where the dependent variable is predicted using two or more independent variables.\n",
    "It fits a linear equation to the data, representing the relationship between  and the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414a607-5cbb-4ac3-be28-dc81b3956cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. What is the main difference between Simple and Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0d230-8f3d-414d-8328-8ac559de4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=The main difference between Simple Linear Regression and Multiple Linear Regression lies in the number \n",
    "of independent variables used to predict the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3952da08-2985-48f9-97e6-2edb09df53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "10.What are the key assumptions of Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d91f6-4e76-414f-8dc8-916bb97c0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Linearity: There is a straight-line relationship between the predictors and the outcome.\n",
    "Independence of Errors: The errors (differences between observed and predicted values) are not related to each other.\n",
    "Homoscedasticity: The spread of errors is the same for all values of the predictors (no larger spread for higher values, for example).\n",
    "Normality of Errors: The errors follow a normal distribution (bell-shaped curve).\n",
    "No Multicollinearity: The predictors are not too highly correlated with each other, meaning one predictor doesn’t predict another too well.\n",
    "No Autocorrelation: The errors are not correlated over time or across observations (important for time series data).\n",
    "Additivity: Each predictor contributes independently to the outcome, and their combined effect is just the sum of their individual effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705ccdc-02ca-48b4-93cd-06959708af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b64e8-69bc-4209-a445-d16936526edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Heteroscedasticity refers to a situation where the variability (or spread) of the residuals (errors) in a regression model is not constant across all \n",
    "levels of the independent variables.In simpler terms, the amount of error in the predictions changes as the value of the predictors increases or decreases.\n",
    "Effects:\n",
    "Inaccurate estimates of standard errors, leading to incorrect p-values.\n",
    "Unreliable confidence intervals.\n",
    "Inefficient predictions and biased results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dbca91-a897-4c25-80b4-2052a1c4c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "12.How can you improve a Multiple Linear Regression model with high multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef40e1-ad8c-4293-ab8a-46ba643144e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=To improve a Multiple Linear Regression model with high multicollinearity, we can do\n",
    "Remove Highly Correlated Predictors,\n",
    "Combine Correlated Predictors,\n",
    "Regularization,\n",
    "Center the Data,\n",
    "Increase Sample Size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2280b-35b0-4493-99fc-648ed4327980",
   "metadata": {},
   "outputs": [],
   "source": [
    "13.What are some common techniques for transforming categorical variables for use in regression models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3424f0b-5498-473a-8920-b317efdb1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Common techniques for transforming categorical variables for use in regression models are:\n",
    "One-Hot Encoding,Label Encoding,Ordinal Encoding,Binary Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec55c59-408e-428a-8b96-068b2ab50731",
   "metadata": {},
   "outputs": [],
   "source": [
    "14.What is the role of interaction terms in Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5bfbb-d9eb-4dc6-9559-781c31aed6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Interaction terms in Multiple Linear Regression capture the combined effect of two or more predictors on the dependent variable. \n",
    "They model situations where the effect of one predictor depends on the value of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e75ce-ff6e-4d07-b01c-e39fc7f26d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f2444-d947-46f0-bb9f-01ea10b77891",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=In Simple Linear Regression, the intercept represents the dependent variable's value when the independent variable is 0.\n",
    "In Multiple Linear Regression, the intercept represents the dependent variable's value when all independent variables are 0, \n",
    "which may not always be meaningful but serves as the model's baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ffa01-f46a-430d-9469-e0a376aaa46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "16.What is the significance of the slope in regression analysis, and how does it affect predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232fa4e-ea97-4d42-a5d0-2cccc9db1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=The slope in regression shows how much the dependent variable changes for a one-unit change in the independent variable.\n",
    "A positive slope means an increase in the predictor leads to an increase in the outcome.\n",
    "A negative slope means an increase in the predictor leads to a decrease in the outcome.\n",
    "It affects predictions by determining how sensitive the outcome is to changes in the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae4c38-79d7-4a55-87e5-8db7edaff8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "17.How does the intercept in a regression model provide context for the relationship between variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a13111-ecf4-4df9-8922-104900650871",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=The intercept in a regression model provides context by showing the predicted value of the dependent variable when all independent variables are set to zero.\n",
    "It serves as a baseline for the model and helps interpret the relationship between variables in terms of where the regression line or plane starts.\n",
    "While the intercept may not always be meaningful (especially if having a value of zero for all predictors is unrealistic), it sets the reference point \n",
    "from which changes in the dependent variable are measured as the predictors vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eecf2d-d462-4fd9-b003-2eabfee4e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "18.What are the limitations of using R² as a sole measure of model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e3d1b-75d5-401e-a440-3098c14257b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Does not indicate causal relationship.\n",
    "Sensitive to outliers.\n",
    "Overfitting: Can increase with additional predictors even if they aren't meaningful.\n",
    "Does not account for model complexity.\n",
    "Does not reflect accuracy (it only shows explained variance, not actual prediction quality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc561bb3-c824-402f-b683-2fd17e13938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "19.How would you interpret a large standard error for a regression coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7877757-125d-46e6-ac03-959b0340c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=A large standard error for a regression coefficient indicates that the estimate of the coefficient is imprecise. This means:\n",
    "The coefficient is uncertain, and there is a high variability in its estimated value across different samples.\n",
    "The t-statistic will be small, leading to less statistical significance (larger p-value).\n",
    "The confidence interval for the coefficient will be wide, meaning we have less confidence in its true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc2bf4-78b4-4fd5-b3e2-e6d49cd47c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "20.How can heteroscedasticity be identified in residual plots, and why is it important to address it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fff109-bd78-4fc1-8076-aedbdc4d4821",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Heteroscedasticity is identified in residual plots by a pattern where the spread of residuals changes (e.g., funnel shape) \n",
    "as predicted values or predictors change.\n",
    "Importance:\n",
    "It causes biased standard errors and unreliable significance tests.\n",
    "It leads to inefficient estimates and incorrect confidence intervals.\n",
    "Addressing it improves model validity and test reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523993f-3b84-4db2-bc38-c5f8d35695ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "21.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76703c2f-1bc0-4da4-b627-3490c9d1cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=If a Multiple Linear Regression model has a high R² but a low adjusted R², it typically means:\n",
    "High R²: The model explains a large proportion of the variance in the dependent variable.\n",
    "Low Adjusted R²: The model may have too many predictors that do not add meaningful information, \n",
    "leading to overfitting. The adjusted R² penalizes the inclusion of irrelevant predictors, and its lower value indicates \n",
    "the model is not improving its explanatory power in proportion to its complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d78f76-faf7-4638-9b2f-3efaf791dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "22.Why is it important to scale variables in Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195dd2a2-4027-470f-841f-e98ca000bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Scaling variables in Multiple Linear Regression ensures:\n",
    "Fairness: Prevents larger-scale variables from dominating the model.\n",
    "Improved Interpretation: Makes it easier to compare coefficients.\n",
    "Efficiency: Helps with regularization and speeds up model convergence.\n",
    "It leads to a more stable and accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b81ca-46ef-440f-abd2-12f730f44a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "23.What is polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785da6f4-6e8f-4e2d-b58e-cb8bbc76204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Polynomial Regression is a type of regression analysis where the relationship between the independent \n",
    "and dependent variables is modeled as an n-th degree polynomial.Instead of fitting a straight line, it fits a curve to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b726af-7f0d-4352-8ba4-b827413a1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "24.How does polynomial regression differ from linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ade9b9-4b77-4a71-bf0f-c5cfe53ce199",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Polynomial Regression fits a curved relationship using higher-degree terms,\n",
    "while Linear Regression fits a straight-line relationship. Polynomial regression is more flexible for non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dae792-4afb-467e-8433-a22162c9c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "25.When is polynomial regression used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2381c-55e2-4cdf-98b7-f5f5fbbb8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Polynomial Regression is used when:\n",
    "The relationship between the dependent and independent variables is non-linear.\n",
    "The data shows a curved pattern that can't be captured by a straight line.\n",
    "You need to model complex trends in the data without adding too many variables.\n",
    "It is helpful when you want to capture higher-order relationships but still keep the model relatively simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4ba63-d0d3-426a-a1a9-3513b00a9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "26.What is the general equation for polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a79c71a-414d-4c3d-82a6-32cc8dd253e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=The general equation for Polynomial Regression is:\n",
    "Y = b0 + b1*X + b2*X^2 + b3*X^3 + ... + bn*X^n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0791e-f446-4e57-ae34-bdb91e36f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "27.Can polynomial regression be applied to multiple variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a510f9-3bba-431d-b860-e14330121c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Yes, Polynomial Regression can be applied to multiple variables. In that case, it becomes Multiple Polynomial Regression, where the model includes \n",
    "polynomial terms for each predictor, as well as interaction terms between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a90fe-cbdf-4bd4-bfd7-ebb6214e6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "28.What are the limitations of polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a14a08c-a475-48da-8c5f-8cbab812b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Limitations of Polynomial Regression:\n",
    "Overfitting: Higher-degree polynomials may fit noise, reducing generalization.\n",
    "Complexity: Models become harder to interpret as degree increases.\n",
    "Extrapolation Issues: Poor predictions outside the data range.\n",
    "Multicollinearity: High-degree terms may be highly correlated, affecting stability.\n",
    "Sensitive to Outliers: Can be distorted by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf2a85-41a6-4fa0-a445-fdda4d8c2415",
   "metadata": {},
   "outputs": [],
   "source": [
    "29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e937c8-8d2f-4c25-9325-2ebfedfd10b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=To evaluate model fit when selecting the degree of a polynomial:\n",
    "Cross-Validation: Test the model on different data subsets.\n",
    "R²/Adjusted R²: Compare goodness of fit, with adjusted R² penalizing complexity.\n",
    "MSE/RMSE: Check prediction accuracy, lower values are better.\n",
    "Residual Analysis: Check for randomness in residuals.\n",
    "Overfitting Check: Compare performance on training vs. test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997bf28-7f10-4fc4-bdd7-8f89e03cfd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "30.Why is visualization important in polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb9074-77a1-4f6e-bfee-6cdabc2bde95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Visualization in polynomial regression helps to:\n",
    "Understand non-linear relationships.\n",
    "Detect overfitting.\n",
    "Assess model fit visually.\n",
    "Compare models for degree selection.\n",
    "It ensures the model captures trends accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1de33-bcce-4548-a0ee-07243519ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "31.How is polynomial regression implemented in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58608ba-13eb-45b5-9f5e-4ab99bdba108",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans=Steps to Implement Polynomial Regression in Python:\n",
    "Import Libraries: Use libraries like numpy, scikit-learn, and matplotlib.\n",
    "Prepare Data: Arrange the data with independent and dependent variables.\n",
    "Transform Features: Use PolynomialFeatures from sklearn to convert data into polynomial terms.\n",
    "Train Model: Fit a LinearRegression model to the transformed polynomial features.\n",
    "Make Predictions: Use the model to predict values.\n",
    "Visualize Results: Plot the original data and the polynomial curve to assess the fit.\n",
    "Evaluate Model: Use metrics like Mean Squared Error (MSE) to check model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
